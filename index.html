<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Evaluating mathematical reasoning of foundation models in visual contexts">
  <meta name="keywords" content="MathVista, Math Vista">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title> WildVideo: Benchmarking LMMs for Understanding Video-Language Interaction</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <!-- <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
/Users/panlu/Library/Mobile Documents/com~apple~CloudDocs/ImageMath/visual-mathqa-server/data_final/images
    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link rel="icon" href="./static/images/mathvista.png">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/leaderboard.css">

  <!-- <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bulma.min.css" rel="stylesheet">
  <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script> -->
  <script type="text/javascript" src="static/js/sort-table.js" defer></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/explorer-index.js"></script>
  <script src="./static/js/question_card.js"></script>

  <script src="./static/js/leaderboard_testmini.js"></script>  
  <script src="./data/results/output_folders.js" defer></script>
  <script src="./data/results/model_scores.js" defer></script>

  <script src="./visualizer/data/data_public.js" defer></script>
</head>

<style>
  #wildvideo-single,
  #wildvideo-multi {
    border-collapse: collapse;
  }

  #wildvideo-single th,
  #wildvideo-single td,
  #wildvideo-multi th,
  #wildvideo-multi td {
    border: 1px solid #999999; 
  }
</style>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <!-- <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a> -->
      <!-- @PAN TODO: consider adding links? -->
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://chameleon-llm.github.io/">
            <b>Chameleon</b> <p style="font-size:18px; display: inline; margin-left: 5px;">üî•</p>
          </a>
          <a class="navbar-item" href="https://scienceqa.github.io/">
            <b>ScienceQA</b> <p style="font-size:18px; display: inline; margin-left: 5px;">üî•</p>
          </a>
          <a class="navbar-item" href="https://github.com/OpenGVLab/LLaMA-Adapter">
            <b>LLaMA-Adapter (V2)</b> <p style="font-size:18px; display: inline; margin-left: 5px;">üî•</p>
          </a>
          <a class="navbar-item" href="https://promptpg.github.io/">
            PromptPG
          </a>
          <a class="navbar-item" href="https://arxiv.org/abs/2307.10635">
            SciBench
          </a>
          <a class="navbar-item" href="https://arxiv.org/abs/2305.12524">
            TheoremQA
          </a>
          <a class="navbar-item" href="https://lila.apps.allenai.org/">
            Lila
          </a>
          <a class="navbar-item" href="https://iconqa.github.io/">
            IconQA
          </a>
          <a class="navbar-item" href="https://lupantech.github.io/inter-gps/">
            Inter-GPS
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title is-bold">
            <img src="static/images/mathvista.png" style="width:1em;vertical-align: middle" alt="Logo"/>
            <span class="mathvista" style="vertical-align: middle">WildVideo</span>
            </h1>
          <h2 class="subtitle is-3 publication-subtitle">
            Benchmarking LMMs for Understanding Video-Language Interaction
            <!-- <br> -->
            <!-- with GPT-4V, Bard, and Other Large Multimodal Models -->
          </h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a>Songyuan Yang</a>,</span>
            <span class="author-block">
              <a>Weijiang Yu</a>,</span>
            <span class="author-block">
              <a>Wenjing Yang</a>,
            </span>
            <span class="author-block">
              <a>Xinwang Liu</a>,
            </span>
            <span class="author-block">
              <a>Huibin Tan</a>,
            </span>
            <span class="author-block">
              <a>Long Lan</a>,
            </span>
            <span class="author-block">
              <a>Nong Xiao</a>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <!-- <span class="author-block"><sup style="color:#6fbf73;">1</sup>University of California, Los Angeles,</span><br>
            <span class="author-block"><sup style="color:#ed4b82">2</sup>University of Washington,</span>
            <span class="author-block"><sup style="color:#ffac33">3</sup>Microsoft Research</span><br> -->
            <span class="paper-block"><b style="color:#f41c1c">IEEE Transactions on Pattern Analysis and Machine Intelligence</b></span>
          </div>
        
          <!-- <section> -->
            <!-- <div class="section" id="org-banners" style="display:fle">
              <a href="https://www.ucla.edu/" target="_blank" rel="external">
                  <img class="center-block org-banner" src="static/images/ucla.png" style="height:3em">
              </a>
              <a href="https://www.washington.edu/" target="blank" class="ext-link">
                  <img class="center-block org-banner" src="static/images/uw.png" style="height:3em">
              </a>
              <a href="https://www.microsoft.com/en-us/research/" target="_blank" rel="external">
                  <img class="center-block org-banner" src="static/images/microsoft.png" style="height:3em">
              </a>
            </div> -->
          <!-- </section> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link.
              <span class="link-block">
                @PAN TODO: change links
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=11097075&tag=1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href="https://ieeexplore.ieee.org/document/11097075"
                   class="external-link button is-normal is-rounded is-dark">
                <!-- <a href="https://lupantech.github.io/papers/arxiv23_mathvista.pdf"
                   class="external-link button is-normal is-rounded is-dark"> -->
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>IEEE</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/yangsongyuan18/WildVideo"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Git Hub</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/yangsongyuan18/wildvideo"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <p style="font-size:18px">ü§ó</p>
                      <!-- üîó -->
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
              <!-- Visualization Link. -->
              <!-- <span class="link-block">
                <a href="https://mathvista.github.io/#visualization"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <p style="font-size:18px">üîÆ</p>
                  </span>
                  <span>Visualize</span>
                </a>
              </span> -->
              <!-- Leaderboard Link. -->
              <!-- <span class="link-block">
                <a href="https://mathvista.github.io/#leaderboard"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <p style="font-size:18px">üèÜ</p>
                  </span>
                  <span>Leaderboard</span>
                </a>
              </span> -->
              <!-- Twitter Link. -->

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="content has-text-centered">
      <img src="static/images/tease_scores_gpt4v.png" alt="geometric reasoning" width="99%"/>
      <p> Accuracy scores of one leading LLM (i.e., PoT GPT-4), four primary LMMs, random chance, and human performance our proposed 
      <img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
      <span class="mathvista">MathVista</span>
      across mathematical reasoning and visual context types. PoT refers to program-of-thought prompting, and PoT GPT-4 is a textual LLM augmented with the caption and OCR text. GPT-4V is manually evaluated via the playground chatbot.
      </p>
    </div>
  </div>
</section> -->


<!-- <section class="section">
  <div class="container" style="margin-top: -150px; margin-bottom: -100px;">
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/tease_scores_version4_gemini.png" alt="geometric reasoning" width="84%"/>
              <p> Accuracy scores of one leading LLM (i.e., PoT GPT-4), four primary LMMs, random chance, and human performance our proposed 
              <img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
              <span class="mathvista">MathVista</span>
              across mathematical reasoning and visual context types. PoT refers to program-of-thought prompting, and PoT GPT-4 is a textual LLM augmented with the caption and OCR text. GPT-4V is manually evaluated via the playground chatbot. <b class="best-score-text" style="color: #C6011F"> The scores of Gemini Ultra are from the Gemini Team, Google.</b>
              </p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/tease_scores_gpt4v.png" alt="geometric reasoning" width="84%"/>
              <p> Accuracy scores of one leading LLM (i.e., PoT GPT-4), four primary LMMs, random chance, and human performance our proposed 
              <img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
              <span class="mathvista">MathVista</span>
              across mathematical reasoning and visual context types. PoT refers to program-of-thought prompting, and PoT GPT-4 is a textual LLM augmented with the caption and OCR text. GPT-4V is manually evaluated via the playground chatbot.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
</section> -->


<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            We introduce WildVideo, an open-world benchmark dataset designed to address how to assess hallucination of Large Multi-modal Models (LMMs) for understanding video-language interaction in the wild. Our WildVideo comprehensively tests the perceptual, cognitive, and contextual comprehension hallucination of LMMs through both single-turn and multi-turn open-ended question-answering (QA) tasks on videos captured from two human perspectives (i.e. first-person view and third-person view). We define 9 distinct tasks that challenge LMMs across multi-level perceptual tasks (e.g., static and dynamic perception), multi-aspect cognitive tasks (e.g., commonsense, world knowledge), and multi-faceted contextual comprehension tasks (e.g., contextual ellipsis, cross-turn retrieval). The benchmark consists of 1,318 meticulously curated videos, supplemented with 13,704 single-turn QA pairs and 1,585 multi-turn dialogues (up to 5 turns). We evaluated 14 commonly-used LMMs on WildVideo, revealing significant hallucination issues of current LMMs, highlighting substantial gaps in their current capabilities.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container">
    
    <div class="columns is-centered">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3" id="leaderboard">Leaderboard on WildVideo (single)</h2>
        <div class="content">
          <p class="mt-3">Accuracy scores on the <b>single</b> subset (13,704 single-turn QA pairs) of 
            <span class="mathvista">WildVideo</span>.
          </p>

          <table class="table is-striped is-hoverable is-fullwidth js-sort-table" id="wildvideo-single">
            <thead>
              <tr>
                <th rowspan="2">Model</th>
                <th colspan="5" class="has-text-centered">Perception</th>
                <th colspan="2" class="has-text-centered">Cognition</th>
                <th rowspan="2">Overall</th>
              </tr>
              <tr>
                <th>Object</th>
                <th>Action</th>
                <th>Visual Loc.</th>
                <th>Consistency</th>
                <th>Causality</th>
                <th>Multimodal Ref.</th>
                <th>World Knowledge</th>
              </tr>
            </thead>

            <tbody>
              <tr>
                <td colspan="9"><b>Open-Source LMMs</b></td>
              </tr>

              <tr>
                <td><b>Mono-InternVL-2B</b></td>
                <td>36.2</td>
                <td>15.6</td>
                <td>22.2</td>
                <td>26.9</td>
                <td>20.7</td>
                <td>11.0</td>
                <td>21.3</td>
                <td>22.0</td>
              </tr>

              <tr>
                <td><b>Video-LLaVA-7B</b></td>
                <td>43.9</td>
                <td>25.8</td>
                <td>23.8</td>
                <td>40.1</td>
                <td>36.9</td>
                <td>27.1</td>
                <td>45.6</td>
                <td>34.7</td>
              </tr>

              <tr>
                <td><b>InternVL2-8B</b></td>
                <td>42.0</td>
                <td>27.9</td>
                <td>33.2</td>
                <td>40.5</td>
                <td>37.4</td>
                <td>28.4</td>
                <td>37.3</td>
                <td>35.2</td>
              </tr>

              <tr>
                <td><b>VideoLLaMA2-7B</b></td>
                <td>52.6</td>
                <td>34.3</td>
                <td>33.0</td>
                <td>46.7</td>
                <td>39.9</td>
                <td>41.9</td>
                <td>48.9</td>
                <td>42.5</td>
              </tr>

              <tr>
                <td><b>Ovis1.6-Gemma-2-9B</b></td>
                <td>58.5</td>
                <td>27.6</td>
                <td>28.6</td>
                <td>41.6</td>
                <td>42.5</td>
                <td>44.0</td>
                <td><b>57.7</b></td>
                <td>42.9</td>
              </tr>

              <tr>
                <td><b>Qwen2-VL-7B</b></td>
                <td>52.5</td>
                <td>30.3</td>
                <td>36.1</td>
                <td>47.8</td>
                <td>47.6</td>
                <td>36.2</td>
                <td>50.9</td>
                <td>43.1</td>
              </tr>

              <tr>
                <td><b>MiniCPM-V2.6-8B</b></td>
                <td>57.4</td>
                <td>39.2</td>
                <td>41.5</td>
                <td>57.3</td>
                <td>51.5</td>
                <td>36.4</td>
                <td>41.8</td>
                <td>46.4</td>
              </tr>

              <tr>
                <td><b>LLaVA-Video-7B-Qwen2</b></td>
                <td><b>63.0</b></td>
                <td><b>50.7</b></td>
                <td><b>47.3</b></td>
                <td><b>58.9</b></td>
                <td><b>60.6</b></td>
                <td><b>47.6</b></td>
                <td><b>46.0</b></td>
                <td><b>53.4</b></td>
              </tr>

              <tr>
                <td colspan="9"><b>Commercial LMMs</b></td>
              </tr>

              <tr>
                <td><b>Claude 3.5 Sonnet</b></td>
                <td>50.0</td>
                <td>43.5</td>
                <td>39.7</td>
                <td>50.5</td>
                <td>36.1</td>
                <td>53.2</td>
                <td>49.8</td>
                <td>46.1</td>
              </tr>

              <tr>
                <td><b>Gemini 1.5 Flash</b></td>
                <td>67.7</td>
                <td>38.2</td>
                <td>42.4</td>
                <td>50.6</td>
                <td>33.6</td>
                <td>49.3</td>
                <td>66.7</td>
                <td>49.8</td>
              </tr>

              <tr>
                <td><b>Gemini 1.5 Pro</b></td>
                <td>67.1</td>
                <td>50.4</td>
                <td>43.6</td>
                <td>59.3</td>
                <td>42.6</td>
                <td>54.7</td>
                <td>57.8</td>
                <td>53.7</td>
              </tr>

              <tr>
                <td><b>GPT-4V</b></td>
                <td>57.0</td>
                <td>39.7</td>
                <td>34.2</td>
                <td>54.9</td>
                <td>39.2</td>
                <td>54.0</td>
                <td>69.7</td>
                <td>49.8</td>
              </tr>

              <tr>
                <td><b>GPT-4o mini</b></td>
                <td>61.4</td>
                <td>45.3</td>
                <td>41.9</td>
                <td>52.9</td>
                <td>52.0</td>
                <td>47.9</td>
                <td>71.8</td>
                <td>53.3</td>
              </tr>

              <tr>
                <td><b>GPT-4o</b></td>
                <td><b>68.2</b></td>
                <td><b>54.2</b></td>
                <td><b>51.5</b></td>
                <td><b>66.5</b></td>
                <td><b>59.2</b></td>
                <td><b>61.4</b></td>
                <td><b>73.6</b></td>
                <td><b>62.1</b></td>
              </tr>
            </tbody>
          </table>

          <p class="is-size-7 has-text-left">
            The best and second-best LMM results are bold and underlined, respectively.
            All numbers are accuracies in %, with a full score of 100%.
          </p>

        <h2 class="title is-3" id="leaderboard-multi">
          Leaderboard on WildVideo (Multi-Turn)
        </h2>
        <p class="mt-3">
          Accuracy (%) on the <b>multi-turn</b> subset (1,585 dialogues, up to 5 turns) of
          <span class="mathvista">WildVideo</span>.
        </p>

        <table class="table is-striped is-hoverable is-fullwidth js-sort-table" id="wildvideo-multi">
          <thead>
            <tr>
              <th rowspan="2">Model</th>
              <th colspan="4" class="has-text-centered">Perception</th>
              <th colspan="3" class="has-text-centered">Cognition</th>
              <th colspan="2" class="has-text-centered">Contextual Comprehension</th>
              <th rowspan="2">Overall</th>
            </tr>
            <tr>
              <th>Object</th>
              <th>Action</th>
              <th>Visual Loc.</th>
              <th>Consistency</th>
              <th>Causality</th>
              <th>Multimodal Ref.</th>
              <th>World Knowledge</th>
              <th>Contextual Ellipsis</th>
              <th>Cross-turn Retrieval</th>
            </tr>
          </thead>

          <tbody>
            <tr>
              <td colspan="11"><b>Open-Source LMMs</b></td>
            </tr>

            <tr>
              <td><b>Mono-InternVL-2B</b></td>
              <td>32.2</td>
              <td>23.4</td>
              <td>17.3</td>
              <td>24.0</td>
              <td>10.8</td>
              <td>14.5</td>
              <td>11.4</td>
              <td>12.1</td>
              <td>6.9</td>
              <td>17.0</td>
            </tr>

            <tr>
              <td><b>InternVL2-8B</b></td>
              <td>31.5</td>
              <td>30.4</td>
              <td>34.8</td>
              <td>48.7</td>
              <td>10.2</td>
              <td>22.0</td>
              <td>25.3</td>
              <td>13.9</td>
              <td>15.2</td>
              <td>25.8</td>
            </tr>

            <tr>
              <td><b>Qwen2-VL-7B</b></td>
              <td>50.7</td>
              <td>34.8</td>
              <td>36.9</td>
              <td><b>55.5</b></td>
              <td><b>44.5</b></td>
              <td>41.9</td>
              <td>38.8</td>
              <td><b>25.3</b></td>
              <td>23.4</td>
              <td>39.1</td>
            </tr>

            <tr>
              <td><b>MiniCPM-V2.6-8B</b></td>
              <td><b>50.9</b></td>
              <td><b>49.6</b></td>
              <td><b>42.8</b></td>
              <td>44.4</td>
              <td>41.3</td>
              <td><b>42.7</b></td>
              <td><b>42.4</b></td>
              <td>23.1</td>
              <td><b>35.7</b></td>
              <td><b>41.4</b></td>
            </tr>

            <tr>
              <td colspan="11"><b>Commercial LMMs</b></td>
            </tr>

            <tr>
              <td><b>Claude 3.5 Sonnet</b></td>
              <td>33.1</td>
              <td>39.3</td>
              <td>24.2</td>
              <td>39.9</td>
              <td>35.3</td>
              <td>35.0</td>
              <td>68.6</td>
              <td>32.6</td>
              <td>30.9</td>
              <td>37.6</td>
            </tr>

            <tr>
              <td><b>Gemini 1.5 Flash</b></td>
              <td>51.7</td>
              <td>40.0</td>
              <td>40.0</td>
              <td>49.9</td>
              <td>34.4</td>
              <td>48.2</td>
              <td>56.0</td>
              <td><b>47.4</b></td>
              <td>46.6</td>
              <td>46.0</td>
            </tr>

            <tr>
              <td><b>Gemini 1.5 Pro</b></td>
              <td>56.2</td>
              <td><b>47.8</b></td>
              <td>43.2</td>
              <td>64.5</td>
              <td>37.5</td>
              <td>44.8</td>
              <td>73.4</td>
              <td>43.1</td>
              <td>37.2</td>
              <td>49.8</td>
            </tr>

            <tr>
              <td><b>GPT-4V</b></td>
              <td>55.3</td>
              <td>41.4</td>
              <td>33.3</td>
              <td>31.8</td>
              <td><b>45.2</b></td>
              <td>48.6</td>
              <td>77.9</td>
              <td>37.1</td>
              <td><b>51.4</b></td>
              <td>46.9</td>
            </tr>

            <tr>
              <td><b>GPT-4o mini</b></td>
              <td>55.0</td>
              <td>38.5</td>
              <td>38.3</td>
              <td>45.2</td>
              <td>40.1</td>
              <td>46.3</td>
              <td>63.1</td>
              <td>30.8</td>
              <td>41.8</td>
              <td>44.3</td>
            </tr>

            <tr>
              <td><b>GPT-4o</b></td>
              <td><b>60.1</b></td>
              <td>45.2</td>
              <td><b>46.2</b></td>
              <td><b>65.7</b></td>
              <td>39.7</td>
              <td><b>50.7</b></td>
              <td><b>78.7</b></td>
              <td>44.5</td>
              <td>43.1</td>
              <td><b>52.7</b></td>
            </tr>
          </tbody>
        </table>

          <b>Human Performance*:</b> Average human performance from AMT annotators who have high school diplomas or above.
          <br>
          <b>Method types:</b> <b>MoE ü§ñ:</b> Mixture of Experts, <b>LMM üñºÔ∏è:</b> Large Multimodal Model, <b>Tool üõ†Ô∏è:</b> Tool-augmented Large Language Model.
          <br>
          <b>Task types:</b> <b>FQA:</b> figure QA,
          <b>GPS:</b> geometry problem solving,
          <b>MWP:</b> math word problem,
          <b>TQA:</b> textbook QA,
          <b>VQA:</b> visual QA.
          <br>
          <b>Math reasoning types:</b> 
          <b>ALG:</b> algebraic,
          <b>ARI:</b> arithmetic,
          <b>GEO:</b> geometry,
          <b>LOG:</b> logical ,
          <b>NUM:</b> numeric,
          <b>SCI:</b> scientific,
          <b>STA:</b> statistical.
          <br>
          <br>
          <div>
          <p>üö® To submit your results to the leaderboard, please send to <a href="mailto:lupantech@gmail.com">this email</a> with your result json files.</p>
          <p>üö® For more submission details, please refer to <a href="https://github.com/lupantech/MathVista?tab=readme-ov-file#-leaderboard-">this link</a> and <a href="https://github.com/lupantech/MathVista?tab=readme-ov-file#-evaluations-on-mathvista">this link</a>.
          </p>
          </div>
        </div>

      </div>
    </div>

  </div>
</section>

<!-- DATASET SECTION -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <!-- <h1 class="title is-1 mathvista"><img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>MathVista Dataset</h1> -->
  <h1 class="title is-1 mathvista">
    <img src="static/images/mathvista.png" style="width:1em;vertical-align: middle" alt="Logo"/>
    <span class="mathvista" style="vertical-align: middle">WildVideo Dataset</span>
  </h1>
  </div>
</section>

<!-- <section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p> -->
            
<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <!-- <div class="column is-full-width has-text-centered"> -->
        <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <p>
            We introduce
            WildVideo, a bilingual benchmark for evaluating hallucinations
            in video-based LMMs, fully aligned with real-world application
            settings. First, WildVideo incorporates multi-turn dialogues,
            enabling the evaluation of contextual understanding and dy-
            namic conversational flows. Second, to mitigate the limitations
            of unimodal references, WildVideo emphasizes the integration
            of multimodal references, such as visual cues paired with tex-
            tual input, which reflect the natural referencing mechanisms
            in human conversations. Furthermore, it incorporates deep vi-
            sual contextual understanding by addressing challenges like
            contextual ellipsis and cross-turn retrieval, which require models to maintain and utilize information across multiple dialogue
            turns. 
            <!-- a compilation of data 1) carefully examined and filtered from 28 existing VQA and MathQA datasets and 2) manually collected by us. In total, 6,141 examples were collected from 31 different datasets. -->
          </p>

          <div id="results-carousel">
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/definitions.png" alt="algebraic reasoning" width="80%"/>
                <p> Task definitions in WildVideo</p>
              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/compare_benchmark.png" alt="arithmetic reasoning" width="50%"/>
                <p> compares WildVideo (ours) with a bunch of existing image and video QA benchmarks along several axes.</p>
              </div>
            </div>
          </div>

          <!-- <div class="content has-text-centered">
            <img src="static/images/source_dataset.png" alt="geometric reasoning" width="60%"/>
            <p> Summary of the 31 different source datasets in <img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
              <span class="mathvista">MathVista</span>.
            </p>
          </div>
          <div class="content has-text-centered">
            <img src="static/images/our_new_3_datasets.png" alt="geometric reasoning" width="70%"/>
            <p> Examples of our newly annotated datasets: IQTest, FunctionQA, and PaperQA.</p>
          </div> -->

          <p>
            All the data examples were divided into two subsets: single and multi. 
            <!-- @PAN TODO: add download links -->
            <ul>
              <li><b>single</b>: 1,318 meticulously curated videos and 13,704 single-turn QA pairs</li>
              <li><b>multi</b>: 1,318 meticulously curated videos and 1,585 multi-turn QA pairs</li>
            </ul>
            You can download the dataset on <a href="https://huggingface.co/datasets/yangsongyuan18/wildvideo" target="_blank">Hugging Face Dataset</a>.
          </p>

        </div>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column" style="margin-right: -20rem;">
        <div class="content has-text-centered">
          <img src="static/images/statistics.png" alt="data-overview" style="max-width: 50%;"/>
          <p> 
            Key statistics of <img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
            <span class="mathvista">WildVideo</span>.<br/>
          </p> 
        </div>
      </div>
      <div class="column">
        <div class="content has-text-centered">
          <img src="static/images/data-composition.png" alt="data-composition" style="max-width: 56%;"/>
          <p>
              Source dataset distribution of <img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
              <span class="mathvista">Wldvideo</span>.<br/>
              <!-- <b>FQA</b>: figure question answering,
              <b>GPS</b>: geometry problem solving,<br/>
              <b>MWP</b>: math word problem,
              <b>TQA</b>: textbook question answering,<br/>
              <b>VQA</b>: visual question answering. -->
          </p>
        </div>
      </div>
    </div>

    
    <!-- <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">Dataset statistics</h2>
        <p>One example for each <b>mathematical reasoning</b> skill required in <img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
          <span class="mathvista">MathVista</span></p>

        <div id="results-carousel" class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/Dataset_statistics/Single_turn_questions.png" alt="arithmetic reasoning" width="60%"/>
              <p>Single-turn questions</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/Dataset_statistics/Multi_turn_questions.png" alt="algebraic reasoning" width="60%"/>
              <p>Multi_turn questions</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/Dataset_statistics/Duration_distribution.png" alt="geometric reasoning" width="60%"/>
              <p>Duration distribution of videos in our benchmark.</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/Dataset_statistics/distribution.png" alt="logical reasoning" width="60%"/>
              <p>Distribution of question counts across turns in multi-turn dialogues.</p>
            </div>
          </div> -->
          <!-- <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/examples/num.png" alt="numeric reasoning" width="60%"/>
              <p>Numeric Reasoning</p>
            </div>
          </div> -->
          <!-- <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/examples/sta.png" alt="statistical reasoning" width="60%"/>
              <p>Statistical Reasoning</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/examples/sci.png" alt="scientific reasoning" width="60%"/>
              <p>Scientific Reasoning</p>
            </div>
          </div> -->

          
        </div>

        
        </div>
      </div>
    </div>
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">EXPERIMENT RESULT</h2>
        <p>Comparison of the capabilities of different LMMs on WildVideo.</p>
        <div id="results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/Experiment/Single_turn.png" alt="context" class="stats-image">
              <p>Performance of Single-turn tasks</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/Experiment/Multi_turn.png" alt="category" class="stats-image"/>
              <p>Performance of Multi-turn tasks</p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
    

<!-- @PAN TODO: bibtex -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 has-text-centered">BibTeX</h2>
    <pre><code>@inproceedings{lu2024mathvista,
  author    = {Lu, Pan and Bansal, Hritik and Xia, Tony and Liu, Jiacheng and Li, Chunyuan and Hajishirzi, Hannaneh and Cheng, Hao and Chang, Kai-Wei and Galley, Michel and Gao, Jianfeng},
  title     = {MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts},
  booktitle={International Conference on Learning Representations (ICLR)},
  year      = {2024}
}</code></pre>
  </div>
</section>

<section>
  <div class="section" id="org-banners" style="display:flex">
    <a href="https://www.ucla.edu/" target="_blank" rel="external">
        <img class="center-block org-banner" src="static/images/ucla.png">
    </a>
    <a href="https://www.washington.edu/" target="blank" class="ext-link">
        <img class="center-block org-banner" src="static/images/uw.png">
    </a>
    <a href="https://www.microsoft.com/en-us/research/" target="_blank" rel="external">
        <img class="center-block org-banner" src="static/images/microsoft.png">
    </a>
  </div>
</section>


<footer class="footer">
  <!-- <div class="container"> -->
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is website adapted from <a href="https://nerfies.github.io/">Nerfies</a>, licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  <!-- </div> -->
</footer>

</body>
</html>
